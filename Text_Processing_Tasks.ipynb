{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fea2726-6f8b-4b85-8cd2-f4d82eee349c",
   "metadata": {},
   "source": [
    "# Most Common Text Processing Tasks in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a129cd-f383-48e4-a46c-1714eb74e412",
   "metadata": {},
   "source": [
    "## Install modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe93750a-c97c-44ef-9b1b-f7018feb8872",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "pip3 -qqq install nltk\n",
    "pip3 -qqq install spacy\n",
    "pip3 -qqq install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030ed99-6501-462e-bbfb-e890bc46d8fd",
   "metadata": {},
   "source": [
    "## Import modules & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c231fee-c49c-4285-9075-6e7a63741d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/zoumanakeita/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import spacy\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9f3524-6274-4ebd-8671-31ec77e58214",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = fetch_20newsgroups(subset='all')\n",
    "articles = news_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc24ad25-b76f-4b0f-833d-98fb403b29e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8264fb20-e3c4-4f0e-b03d-4d95b8285bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(articles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791ed3a-f327-4aad-b8a8-3d5368314e27",
   "metadata": {},
   "source": [
    "# Processing Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3088d3df-caa6-4e20-8a9b-4e6c575214ae",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fbdfcaf-5e50-470a-a155-fc1e0d3ba314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68250abe-ab3d-44cb-a4e4-3d1283e86cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_article = \"\"\" \n",
    "I am sure some bashers of Pens fans are pretty confused about the lack\n",
    "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
    "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
    "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
    "are killing those Devils worse than I thought. Jagr just showed you why\n",
    "he is much better than his regular season stats. He is also a lot\n",
    "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
    "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
    "regular season game.          PENS RULE!!!\n",
    "\"\"\"\n",
    "\n",
    "word_tokens = word_tokenize(first_article)\n",
    "sentence_tokens = sent_tokenize(first_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f382536-25d6-4ae7-9185-a634d7641c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'sure', 'some', 'bashers', 'of', 'Pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about', 'the', 'recent', 'Pens', 'massacre', 'of', 'the', 'Devils', '.', 'Actually', ',', 'I', 'am', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved', '.', 'However', ',', 'I', 'am', 'going', 'to', 'put', 'an', 'end', 'to', 'non-PIttsburghers', \"'\", 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'Pens', '.', 'Man', ',', 'they', 'are', 'killing', 'those', 'Devils', 'worse', 'than', 'I', 'thought', '.', 'Jagr', 'just', 'showed', 'you', 'why', 'he', 'is', 'much', 'better', 'than', 'his', 'regular', 'season', 'stats', '.', 'He', 'is', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoffs', '.', 'Bowman', 'should', 'let', 'JAgr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'games', 'since', 'the', 'Pens', 'are', 'going', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'Jersey', 'anyway', '.', 'I', 'was', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'Islanders', 'lose', 'the', 'final', 'regular', 'season', 'game', '.', 'PENS', 'RULE', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0262932-4ec1-42a1-bb74-88edf4373650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils.', 'Actually,\\nI am  bit puzzled too and a bit relieved.', \"However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens.\", 'Man, they\\nare killing those Devils worse than I thought.', 'Jagr just showed you why\\nhe is much better than his regular season stats.', 'He is also a lot\\nfo fun to watch in the playoffs.', 'Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway.', 'I was very disappointed not to see the Islanders lose the final\\nregular season game.', 'PENS RULE!!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d82a588-70a9-4e48-89b1-de70866d2e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils.\n",
      "\n",
      "\n",
      "Actually,\n",
      "I am  bit puzzled too and a bit relieved.\n",
      "\n",
      "\n",
      "However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens.\n",
      "\n",
      "\n",
      "Man, they\n",
      "are killing those Devils worse than I thought.\n",
      "\n",
      "\n",
      "Jagr just showed you why\n",
      "he is much better than his regular season stats.\n",
      "\n",
      "\n",
      "He is also a lot\n",
      "fo fun to watch in the playoffs.\n",
      "\n",
      "\n",
      "Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway.\n",
      "\n",
      "\n",
      "I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.\n",
      "\n",
      "\n",
      "PENS RULE!!\n",
      "\n",
      "\n",
      "!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in sentence_tokens:\n",
    "    print(sent)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64596ec-5765-4d2b-a767-6bf14f1032fe",
   "metadata": {},
   "source": [
    "## Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "848a0059-158e-4f60-b59e-ab800ec94de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0424ed4d-f484-4716-81a7-51417b6788ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire the stop words\n",
    "english_stw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01d30914-4667-49be-9010-f86b4bc345fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'sure', 'bashers', 'Pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'Pens', 'massacre', 'Devils', '.', 'Actually', ',', 'I', 'bit', 'puzzled', 'bit', 'relieved', '.', 'However', ',', 'I', 'going', 'put', 'end', 'non-PIttsburghers', \"'\", 'relief', 'bit', 'praise', 'Pens', '.', 'Man', ',', 'killing', 'Devils', 'worse', 'I', 'thought', '.', 'Jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats', '.', 'He', 'also', 'lot', 'fo', 'fun', 'watch', 'playoffs', '.', 'Bowman', 'let', 'JAgr', 'lot', 'fun', 'next', 'couple', 'games', 'since', 'Pens', 'going', 'beat', 'pulp', 'Jersey', 'anyway', '.', 'I', 'disappointed', 'see', 'Islanders', 'lose', 'final', 'regular', 'season', 'game', '.', 'PENS', 'RULE', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "non_stop_words = [word for word in word_tokens if word not in english_stw]\n",
    "print(non_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56e87e7-606a-4fdd-9f68-3c8ded31ee2e",
   "metadata": {},
   "source": [
    "## Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4abf064-bd18-484e-87ba-6925b2e9dc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'sure', 'bashers', 'Pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'Pens', 'massacre', 'Devils', 'Actually', 'I', 'bit', 'puzzled', 'bit', 'relieved', 'However', 'I', 'going', 'put', 'end', 'non-PIttsburghers', 'relief', 'bit', 'praise', 'Pens', 'Man', 'killing', 'Devils', 'worse', 'I', 'thought', 'Jagr', 'showed', 'much', 'better', 'regular', 'season', 'stats', 'He', 'also', 'lot', 'fo', 'fun', 'watch', 'playoffs', 'Bowman', 'let', 'JAgr', 'lot', 'fun', 'next', 'couple', 'games', 'since', 'Pens', 'going', 'beat', 'pulp', 'Jersey', 'anyway', 'I', 'disappointed', 'see', 'Islanders', 'lose', 'final', 'regular', 'season', 'game', 'PENS', 'RULE']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "without_punct = list(filter(lambda word: word not in string.punctuation, non_stop_words))\n",
    "\n",
    "print(without_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24914af8-2d03-4bbd-b80e-656c89dc5a96",
   "metadata": {},
   "source": [
    "## Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ac1f4f0-5b5b-4165-879a-a98b51b7a1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zoumanakeita/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/zoumanakeita/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"This thing really confuses. \n",
    "                 But you confuse me more than what is written here.  \n",
    "                 So stay away from explaining things you do not understand. \n",
    "              \"\"\"\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Download wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Instanciate Lemmatizer\n",
    "my_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create instance of stemmer\n",
    "my_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def stem_words(sentence, model=my_stemmer):\n",
    "  \n",
    "    for word in sentence.split():\n",
    "        stem = model.stem(word)\n",
    "        print(\"Word: {} ---> : {}\".format(word, stem))\n",
    "\n",
    "def lemmatize_words(sentence, model = my_lemmatizer):\n",
    "\n",
    "    for word in sentence.split():\n",
    "        lemma = model.lemmatize(word)\n",
    "        print(\"Word: {} ---> : {}\".format(word, lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "605f1860-9832-40f7-8115-995ea53954f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: This ---> : thi\n",
      "Word: thing ---> : thing\n",
      "Word: really ---> : realli\n",
      "Word: confuses. ---> : confuses.\n",
      "Word: But ---> : but\n",
      "Word: you ---> : you\n",
      "Word: confuse ---> : confus\n",
      "Word: me ---> : me\n",
      "Word: more ---> : more\n",
      "Word: than ---> : than\n",
      "Word: what ---> : what\n",
      "Word: is ---> : is\n",
      "Word: written ---> : written\n",
      "Word: here. ---> : here.\n",
      "Word: So ---> : so\n",
      "Word: stay ---> : stay\n",
      "Word: away ---> : away\n",
      "Word: from ---> : from\n",
      "Word: explaining ---> : explain\n",
      "Word: things ---> : thing\n",
      "Word: you ---> : you\n",
      "Word: do ---> : do\n",
      "Word: not ---> : not\n",
      "Word: understand. ---> : understand.\n"
     ]
    }
   ],
   "source": [
    "stem_words(sample_text, model=my_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecb74ad9-8a2f-420e-9532-8e3573716718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: This ---> : This\n",
      "Word: thing ---> : thing\n",
      "Word: really ---> : really\n",
      "Word: confuses. ---> : confuses.\n",
      "Word: But ---> : But\n",
      "Word: you ---> : you\n",
      "Word: confuse ---> : confuse\n",
      "Word: me ---> : me\n",
      "Word: more ---> : more\n",
      "Word: than ---> : than\n",
      "Word: what ---> : what\n",
      "Word: is ---> : is\n",
      "Word: written ---> : written\n",
      "Word: here. ---> : here.\n",
      "Word: So ---> : So\n",
      "Word: stay ---> : stay\n",
      "Word: away ---> : away\n",
      "Word: from ---> : from\n",
      "Word: explaining ---> : explaining\n",
      "Word: things ---> : thing\n",
      "Word: you ---> : you\n",
      "Word: do ---> : do\n",
      "Word: not ---> : not\n",
      "Word: understand. ---> : understand.\n"
     ]
    }
   ],
   "source": [
    "lemmatize_words(sample_text, model = my_lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e81ed-8196-4feb-9e7b-8bdf6736b2ae",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "988247a6-2803-4602-97d9-ff04bac12d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zoumanakeita/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5be857f-dce0-4ea9-a59c-ef0d34b40d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('sure', 'VBP'), ('bashers', 'NNS'), ('Pens', 'NNPS'), ('fans', 'NNS'), ('pretty', 'RB'), ('confused', 'JJ'), ('lack', 'NN'), ('kind', 'NN'), ('posts', 'VBZ'), ('recent', 'JJ'), ('Pens', 'NNP'), ('massacre', 'NN'), ('Devils', 'NNP'), ('Actually', 'NNP'), ('I', 'PRP'), ('bit', 'VBP'), ('puzzled', 'JJ'), ('bit', 'NN'), ('relieved', 'VBD'), ('However', 'RB'), ('I', 'PRP'), ('going', 'VBG'), ('put', 'JJ'), ('end', 'VB'), ('non-PIttsburghers', 'NNS'), ('relief', 'JJ'), ('bit', 'NN'), ('praise', 'NN'), ('Pens', 'NNP'), ('Man', 'NNP'), ('killing', 'VBG'), ('Devils', 'NNP'), ('worse', 'NN'), ('I', 'PRP'), ('thought', 'VBD'), ('Jagr', 'NNP'), ('showed', 'VBD'), ('much', 'JJ'), ('better', 'JJR'), ('regular', 'JJ'), ('season', 'NN'), ('stats', 'NNS'), ('He', 'PRP'), ('also', 'RB'), ('lot', 'VBD'), ('fo', 'JJ'), ('fun', 'NN'), ('watch', 'NN'), ('playoffs', 'NNS'), ('Bowman', 'NNP'), ('let', 'VBP'), ('JAgr', 'NNP'), ('lot', 'NN'), ('fun', 'NN'), ('next', 'JJ'), ('couple', 'NN'), ('games', 'NNS'), ('since', 'IN'), ('Pens', 'NNP'), ('going', 'VBG'), ('beat', 'NN'), ('pulp', 'NN'), ('Jersey', 'NNP'), ('anyway', 'RB'), ('I', 'PRP'), ('disappointed', 'VBD'), ('see', 'NN'), ('Islanders', 'NNPS'), ('lose', 'VBP'), ('final', 'JJ'), ('regular', 'NN'), ('season', 'NN'), ('game', 'NN'), ('PENS', 'NNP'), ('RULE', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = pos_tag(without_punct)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8621590-20c8-45b9-a864-ec1a7c467b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
